![CI](https://github.com/tayo4christ/makaton-gesture-recognition/actions/workflows/ci.yml/badge.svg)

# Makaton Gesture Recognition Tool ü§ñ‚úã
[![Build](https://img.shields.io/badge/build-passing-brightgreen)](#)
[![License](https://img.shields.io/badge/license-MIT-blue)](#)
[![Coverage](https://img.shields.io/badge/coverage-80%25-green)](#)
[![Python](https://img.shields.io/badge/python-3.10%2B-yellow)](#)

---

[![Status](https://img.shields.io/badge/status-active-success.svg)](#)
[![Research](https://img.shields.io/badge/research-AI%20%26%20Education-blue.svg)](#)
[![Made with Python](https://img.shields.io/badge/made%20with-Python%203.10+-yellow.svg)](#)
[![Maintained](https://img.shields.io/badge/maintained-yes-green.svg)](#)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](#)

---

## üåç Overview

This project provides a **real-time hand gesture recognition system** tailored to translate basic **Makaton gestures into English text**.
Built with computer vision and deep learning technologies, it enables **inclusive communication** for individuals with speech or language difficulties.

This innovation combines **computer vision and assistive AI** to make communication tools more inclusive and accessible for learners and individuals with speech challenges.

---

## üîç Problem It Solves

Many individuals who rely on Makaton for communication face barriers when interacting with non-Makaton users.
This tool bridges that gap by recognizing predefined hand gestures and translating them into English in real time, helping **Makaton and non-Makaton users** communicate more effectively.

---

## üë• Intended Users

- Makaton users
- Educators and support staff working with individuals with communication difficulties
- Researchers and developers exploring gesture-based communication systems

---

## üõ†Ô∏è Technologies Used

- **Python**
- **OpenCV**
- **MediaPipe**
- **NumPy**
- **Tkinter** (for GUI)
- **Pillow** (for image display)

---

## ‚öôÔ∏è System Architecture

Video Input ‚ûú Frame Extraction ‚ûú Hand Landmark Detection ‚ûú Gesture Classification ‚ûú English Translation ‚ûú Display on GUI

---

## üì• Input / Output

**Input:**
Live webcam stream of hand gestures

**Output:**
Recognized Makaton gesture and corresponding English translation displayed on screen

---

## üíª Usage Instructions

### üß∞ Prerequisites
Ensure you have Python 3.10 or higher installed, then install dependencies:

```bash
pip install opencv-python mediapipe numpy pillow

```

Running the Application
```bash
git clone https://github.com/tayo4christ/makaton-gesture-recognition.git
cd makaton-gesture-recognition
python makaton_recognition.py
```

Use your webcam to perform gestures. The GUI will display the detected gesture and its translation.
---

## ‚öôÔ∏è Developer Setup

If you‚Äôd like to contribute or run the project in development mode, follow these steps:

### 1Ô∏è‚É£ Create and activate a virtual environment

```bash
python -m venv .venv
# On Windows
.\.venv\Scripts\Activate
# On macOS/Linux
source .venv/bin/activate
```

2Ô∏è‚É£ Install dependencies
```bash
pip install -r requirements.txt
```
3Ô∏è‚É£ Run pre-commit hooks locally (for formatting and linting)
```bash
pre-commit install
pre-commit run --all-files
```

4Ô∏è‚É£ Run tests
```bash
pytest -q
```
---

## ‚úÖ Field Testing

The prototype was **tested with 8 students at Derby Cathedral School**, Derby, UK.
Feedback showed the interface was intuitive and translations were accurate for supported gestures.
This demonstrates **early-stage validation** and **real-world relevance** ‚Äî essential for inclusive AI systems.

---

## üß† Recognized Gestures and Descriptions

| Gesture   | Description                                                          |
|------------|----------------------------------------------------------------------|
| Hello      | Open hand, palm facing forward, all fingers extended                 |
| Goodbye    | Open hand, palm facing forward, moving fingers as if waving          |
| Please     | Flat hand, palm facing up, moving in a small circular motion         |
| Thank You  | Flat hand, palm facing up, moving away from the chin                 |
| Yes        | Fist with thumb up                                                   |

---

## üì∏ Screenshots / Demo

üé¨ Demo video and screenshots available in the `media/` folder (add yours there).
Example:

![App Interface](media/demo-sample.png)

---

## üß± Project Structure

```text
makaton-gesture-recognition/
‚îÇ
‚îú‚îÄ‚îÄ src/                  # Core source code
‚îú‚îÄ‚îÄ data/                 # Datasets
‚îú‚îÄ‚îÄ models/               # Model weights
‚îú‚îÄ‚îÄ notebooks/            # Research experiments
‚îú‚îÄ‚îÄ tests/                # Unit tests
‚îú‚îÄ‚îÄ media/                # Demo videos & screenshots
‚îî‚îÄ‚îÄ README.md             # Project documentation
```
---

## üó∫Ô∏è Roadmap

- [x] Initial prototype (gesture-to-text)
- [ ] Add dynamic Makaton gesture recognition (temporal analysis)
- [ ] Integrate English text-to-speech output
- [ ] Expand dataset beyond 10 core gestures
- [ ] Deploy a web demo using Streamlit or Gradio

---

## ü§ù Contributions

Contributions are welcome!
You can help by:

- Improving gesture detection accuracy
- Adding new signs
- Enhancing the GUI and accessibility features

Before contributing, please read our [CONTRIBUTING.md](CONTRIBUTING.md) guide for details on:
- Project setup and environment
- Code style and commit conventions
- How to open pull requests or report issues

Create a pull request or open an issue ‚Äî let‚Äôs build accessible AI together üí™.

---

## üß† Author

**Omotayo Omoyemi**
MSc in Computer Science | Researcher in AI & Education

- **LinkedIn:** [https://www.linkedin.com/in/omotayo-emmanuel-omoyemi-mbcs-054484191/](https://www.linkedin.com/in/omotayo-emmanuel-omoyemi-mbcs-054484191/)
- **FreeCodeCamp Publications:** [https://www.freecodecamp.org/news/author/tayo4christ/](https://www.freecodecamp.org/news/author/tayo4christ/)
- **ACM Publication:** [https://dl.acm.org/doi/10.1145/3708635.3708647](https://dl.acm.org/doi/10.1145/3708635.3708647)

---

## üß© Version
**Current Release:** `v1.0.0`
Stable foundation with CI/CD, pre-commit hooks, Ruff linting, and auto-formatting.
Next milestone: implement gesture dataset expansion and text-to-speech.

---

## üåê Community & Code of Conduct

We‚Äôre committed to fostering an open, welcoming, and harassment-free community for everyone.
Please read our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing.

[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](https://www.contributor-covenant.org/)

---

## üîç Project Insights

This project is part of ongoing research on **AI-assisted communication systems**, focusing on translating Makaton gestures into English.
It explores **computer vision**, **gesture recognition**, and **assistive technology for inclusion**.

Future research directions include:
- Integrating speech synthesis for full gesture-to-speech communication
- Improving temporal recognition of dynamic gestures
- Expanding datasets for better generalization
- Exploring multimodal inputs (e.g., face expressions, context awareness)

---

**Research Focus:** Inclusive AI ‚Ä¢ Assistive Technology ‚Ä¢ Computer Vision ‚Ä¢ NLP Integration

---

## üìö Citations & References

If you reference or build upon this project in academic or research work, please cite:

> S. Karkalas, Omoyemi O. (2024). *Designing a tool that automatically translate Makaton signs from Live video streams into English.*
> In Proceedings of the 13th International Conference on Software and Information Engineering (ICSIE 2024).
> DOI: [10.1145/3708635.3708647](https://dl.acm.org/doi/10.1145/3708635.3708647)

This repository also draws inspiration from:
- Google Mediapipe Hand Tracking documentation
- OpenCV real-time video analysis guides
- Python Tkinter GUI frameworks for accessibility tools

---

## üôè Acknowledgements

Special thanks to the open-source community projects and mentors who inspired this work:
- **MediaPipe** by Google for providing accessible hand-tracking tools
- **OpenCV** community for continuous innovation in computer vision
- **Dr. Sokratis Karkalas** for research guidance and collaboration during ICSIE 2024
- All educators and students who provided early feedback during prototype testing

---

## üìú License

This project is open-source and available under the **MIT License**.
