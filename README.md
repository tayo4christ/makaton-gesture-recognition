![CI](https://github.com/tayo4christ/makaton-gesture-recognition/actions/workflows/ci.yml/badge.svg)

# Makaton Gesture Recognition Tool ðŸ¤–âœ‹
[![Build](https://img.shields.io/badge/build-passing-brightgreen)](#)
[![License](https://img.shields.io/badge/license-MIT-blue)](#)
[![Coverage](https://img.shields.io/badge/coverage-80%25-green)](#)
[![Python](https://img.shields.io/badge/python-3.10%2B-yellow)](#)

---

[![Status](https://img.shields.io/badge/status-active-success.svg)](#)
[![Research](https://img.shields.io/badge/research-AI%20%26%20Education-blue.svg)](#)
[![Made with Python](https://img.shields.io/badge/made%20with-Python%203.10+-yellow.svg)](#)
[![Maintained](https://img.shields.io/badge/maintained-yes-green.svg)](#)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](#)

---

## ðŸŒ Overview

This project provides a **real-time hand gesture recognition system** tailored to translate basic **Makaton gestures into English text**.
Built with computer vision and deep learning technologies, it enables **inclusive communication** for individuals with speech or language difficulties.

This innovation combines **computer vision and assistive AI** to make communication tools more inclusive and accessible for learners and individuals with speech challenges.

---

## ðŸ” Problem It Solves

Many individuals who rely on Makaton for communication face barriers when interacting with non-Makaton users.
This tool bridges that gap by recognizing predefined hand gestures and translating them into English in real time, helping **Makaton and non-Makaton users** communicate more effectively.

---

## ðŸ‘¥ Intended Users

- Makaton users
- Educators and support staff working with individuals with communication difficulties
- Researchers and developers exploring gesture-based communication systems

---

## ðŸ› ï¸ Technologies Used

- **Python**
- **OpenCV**
- **MediaPipe**
- **NumPy**
- **Tkinter** (for GUI)
- **Pillow** (for image display)

---

## âš™ï¸ System Architecture

Video Input âžœ Frame Extraction âžœ Hand Landmark Detection âžœ Gesture Classification âžœ English Translation âžœ Display on GUI

---

## ðŸ“¥ Input / Output

**Input:**
Live webcam stream of hand gestures

**Output:**
Recognized Makaton gesture and corresponding English translation displayed on screen

---

## ðŸ’» Usage Instructions

### ðŸ§° Prerequisites
Ensure you have Python 3.10 or higher installed, then install dependencies:

```bash
pip install opencv-python mediapipe numpy pillow

```

Running the Application
```bash
git clone https://github.com/tayo4christ/makaton-gesture-recognition.git
cd makaton-gesture-recognition
python makaton_recognition.py
```

Use your webcam to perform gestures. The GUI will display the detected gesture and its translation.
---

## âš™ï¸ Developer Setup

If youâ€™d like to contribute or run the project in development mode, follow these steps:

### 1ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv .venv
# On Windows
.\.venv\Scripts\Activate
# On macOS/Linux
source .venv/bin/activate
```

2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
3ï¸âƒ£ Run pre-commit hooks locally (for formatting and linting)
```bash
pre-commit install
pre-commit run --all-files
```

4ï¸âƒ£ Run tests
```bash
pytest -q
```
---

## âœ… Field Testing

The prototype was **tested with 8 students at Derby Cathedral School**, Derby, UK.
Feedback showed the interface was intuitive and translations were accurate for supported gestures.
This demonstrates **early-stage validation** and **real-world relevance** â€” essential for inclusive AI systems.

---

## ðŸ§  Recognized Gestures and Descriptions

| Gesture   | Description                                                          |
|------------|----------------------------------------------------------------------|
| Hello      | Open hand, palm facing forward, all fingers extended                 |
| Goodbye    | Open hand, palm facing forward, moving fingers as if waving          |
| Please     | Flat hand, palm facing up, moving in a small circular motion         |
| Thank You  | Flat hand, palm facing up, moving away from the chin                 |
| Yes        | Fist with thumb up                                                   |

---

## ðŸ“¸ Screenshots / Demo

ðŸŽ¬ Demo video and screenshots available in the `media/` folder (add yours there).
Example:

![App Interface](media/demo-sample.png)

---

## ðŸ§± Project Structure

```text
makaton-gesture-recognition/
â”‚
â”œâ”€â”€ src/                  # Core source code
â”œâ”€â”€ data/                 # Datasets
â”œâ”€â”€ models/               # Model weights
â”œâ”€â”€ notebooks/            # Research experiments
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ media/                # Demo videos & screenshots
â””â”€â”€ README.md             # Project documentation
```
---

## ðŸ—ºï¸ Roadmap

- [x] Initial prototype (gesture-to-text)
- [ ] Add dynamic Makaton gesture recognition (temporal analysis)
- [ ] Integrate English text-to-speech output
- [ ] Expand dataset beyond 10 core gestures
- [ ] Deploy a web demo using Streamlit or Gradio

---

## ðŸ¤ Contributions

Contributions are welcome!
You can help by:

- Improving gesture detection accuracy
- Adding new signs
- Enhancing the GUI and accessibility features

Before contributing, please read our [CONTRIBUTING.md](CONTRIBUTING.md) guide for details on:
- Project setup and environment
- Code style and commit conventions
- How to open pull requests or report issues

Create a pull request or open an issue â€” letâ€™s build accessible AI together ðŸ’ª.

---

## ðŸ§  Author

**Omotayo Omoyemi**
MSc in Computer Science | Researcher in AI & Education

- **LinkedIn:** [https://www.linkedin.com/in/omotayo-emmanuel-omoyemi-mbcs-054484191/](https://www.linkedin.com/in/omotayo-emmanuel-omoyemi-mbcs-054484191/)
- **FreeCodeCamp Publications:** [https://www.freecodecamp.org/news/author/tayo4christ/](https://www.freecodecamp.org/news/author/tayo4christ/)
- **ACM Publication:** [https://dl.acm.org/doi/10.1145/3708635.3708647](https://dl.acm.org/doi/10.1145/3708635.3708647)

---

## ðŸ§© Version
**Current Release:** `v1.0.0`
Stable foundation with CI/CD, pre-commit hooks, Ruff linting, and auto-formatting.
Next milestone: implement gesture dataset expansion and text-to-speech.

---

## ðŸŒ Community & Code of Conduct

Weâ€™re committed to fostering an open, welcoming, and harassment-free community for everyone.
Please read our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing.

[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](https://www.contributor-covenant.org/)

---

## ðŸ” Project Insights

This project is part of ongoing research on **AI-assisted communication systems**, focusing on translating Makaton gestures into English.
It explores **computer vision**, **gesture recognition**, and **assistive technology for inclusion**.

Future research directions include:
- Integrating speech synthesis for full gesture-to-speech communication
- Improving temporal recognition of dynamic gestures
- Expanding datasets for better generalization
- Exploring multimodal inputs (e.g., face expressions, context awareness)

---

**Research Focus:** Inclusive AI â€¢ Assistive Technology â€¢ Computer Vision â€¢ NLP Integration

---

## ðŸ“š Citations & References

If you reference or build upon this project in academic or research work, please cite:

> S. Karkalas, Omoyemi O. (2024). *Designing a tool that automatically translate Makaton signs from Live video streams into English.*
> In Proceedings of the 13th International Conference on Software and Information Engineering (ICSIE 2024).
> DOI: [10.1145/3708635.3708647](https://dl.acm.org/doi/10.1145/3708635.3708647)

This repository also draws inspiration from:
- Google Mediapipe Hand Tracking documentation
- OpenCV real-time video analysis guides
- Python Tkinter GUI frameworks for accessibility tools

---

## ðŸ“œ License

This project is open-source and available under the **MIT License**.
