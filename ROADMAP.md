# 🗺️ Makaton Gesture Recognition — Roadmap

This document outlines planned milestones for the Makaton Gesture Recognition System,
a computer vision–based AI tool for translating Makaton gestures into English text.

---

## ✅ Current Progress
- Prototype GUI built using Python, OpenCV, and MediaPipe.
- Basic static gesture recognition functional (Hello, Goodbye, Please, Thank You, Yes).
- Integrated early feedback from Derby Cathedral School testing.
- Pre-commit hooks, linting, and CI setup on GitHub Actions.
- Intellectual Property filing submitted to the UK Intellectual Property Office (awaiting payment).

---

## 🧠 Next Development Milestones

### 🔹 Phase 1: Dataset & Model
- Collect extended Makaton gesture dataset.
- Label samples for 20–30 common gestures.
- Train CNN-based classifier for dynamic gestures.

### 🔹 Phase 2: Translation Layer
- Map recognized gestures to full English phrases.
- Integrate optional speech synthesis (Text-to-Speech).

### 🔹 Phase 3: Accessibility Features
- Add larger buttons and captions for low-vision users.
- Integrate keyboard shortcuts for educators.

### 🔹 Phase 4: Web Deployment
- Deploy live demo using Streamlit or Gradio.
- Enable webcam access directly in browser.

---

## 📅 Long-Term Vision
Develop a multilingual Makaton-to-English translation suite that supports gesture, symbol, and speech integration.
Potential collaborations: NHS communication support units, SEND schools, and inclusive education technology hubs.

---

## 👩‍💻 Maintainer
**Omotayo Omoyemi**
MSc in Computer Science | AI & Education Researcher
